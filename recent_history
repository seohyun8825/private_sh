Welcome to your vast.ai container! This session is running in `tmux`.
To disconnect without closing your processes, press ctrl+b, release, then d.
To disable auto-tmux, run `touch ~/.no_auto_tmux` and reconnect. See also https://tmuxcheatsheet.com/
.com/YangLing0818/RPG-DiffusionMaster.gi^C
(base) root@C.13666495:/home/kubig/RPG-DiffusionMaster$ python RPG.py
Traceback (most recent call last):
  File "RPG.py", line 2, in <module>
    from RegionalDiffusion_xl import RegionalDiffusionXLPipeline
  File "/home/kubig/RPG-DiffusionMaster/RegionalDiffusion_xl.py", line 22, in <module>
    from cross_attention import hook_forwards,TOKENS,TOKENSCON
  File "/home/kubig/RPG-DiffusionMaster/cross_attention.py", line 7, in <module>
    import xformers
ModuleNotFoundError: No module named 'xformers'
(base) root@C.13666495:/home/kubig/RPG-DiffusionMaster$ conda activate RPG
(RPG) root@C.13666495:/home/kubig/RPG-DiffusionMaster$ python RPG>py
python: can't open file '/home/kubig/RPG-DiffusionMaster/RPG': [Errno 2] No such file or directory
(RPG) root@C.13666495:/home/kubig/RPG-DiffusionMaster$ python RPG.py
Loading pipeline components...: 100%|██████████████████| 7/7 [00:02<00:00,  2.57it/s]
Using hardcoded values instead of GPT API.
Final split ratio: 1,2,1;1,1,1
Regional Prompt: Captures the dog with an engaging head tilt, eyes bright with curiosity, and a playful grin, with the letter Thumbnail in the
left top part. BREAK Shows the dog lying down, paws folded, with a calm expression, its fur slightly ruffled. BREAK Presents the dog mid-play,
leaping with ears perked up, full of energetic joy. BREAK Features the dog sitting serenely, a gentle breeze rustling its fur, with a calm and
serene gaze.
split 1,2,1;1,1,1
split_ratio2r [1.0, 1.0]
split_ratio2 [[2.0, 1.0], [1.0, 1.0]]
[[0.5, 0.5], [0.5, 0.5]]
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
5
prompt_embeds shape torch.Size([1, 385, 2048])
split: [['1', '2', '1'], ['1', '1', '1']]
Combined image saved at: visualize/combined_image.png
Traceback (most recent call last):
  File "/home/kubig/RPG-DiffusionMaster/RPG.py", line 29, in <module>
    images = pipe(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/kubig/RPG-DiffusionMaster/RegionalDiffusion_xl.py", line 1307, in __call__
    image_embeds, uncond_image_embeds = self.regional_encode_image(
  File "/home/kubig/RPG-DiffusionMaster/RegionalDiffusion_xl.py", line 601, in regional_encode_image
    image_embeds = torch.cat(image_embeds_list, dim=1)
RuntimeError: torch.cat(): expected a non-empty list of Tensors
(RPG) root@C.13666495:/home/kubig/RPG-DiffusionMaster$ python RPG.py
Loading pipeline components...: 100%|██████████████████| 7/7 [00:02<00:00,  2.88it/s]
Using hardcoded values instead of GPT API.
Final split ratio: 1,2,1;1,1,1
Regional Prompt: Captures the dog with an engaging head tilt, eyes bright with curiosity, and a playful grin, with the letter Thumbnail in the
left top part. BREAK Shows the dog lying down, paws folded, with a calm expression, its fur slightly ruffled. BREAK Presents the dog mid-play,
leaping with ears perked up, full of energetic joy. BREAK Features the dog sitting serenely, a gentle breeze rustling its fur, with a calm and
serene gaze.
split 1,2,1;1,1,1
split_ratio2r [1.0, 1.0]
split_ratio2 [[2.0, 1.0], [1.0, 1.0]]
[[0.5, 0.5], [0.5, 0.5]]
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
5
prompt_embeds shape torch.Size([1, 385, 2048])
split: [['1', '2', '1'], ['1', '1', '1']]
Combined image saved at: visualize/combined_image.png
Images inside regional_encode_image: []
Length of image_embeds_list: 0
Length of uncond_image_embeds_list: 0
Traceback (most recent call last):
  File "/home/kubig/RPG-DiffusionMaster/RPG.py", line 29, in <module>
    images = pipe(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/kubig/RPG-DiffusionMaster/RegionalDiffusion_xl.py", line 1316, in __call__
    image_embeds, uncond_image_embeds = self.regional_encode_image(
  File "/home/kubig/RPG-DiffusionMaster/RegionalDiffusion_xl.py", line 610, in regional_encode_image
    image_embeds = torch.cat(image_embeds_list, dim=1)
RuntimeError: torch.cat(): expected a non-empty list of Tensors
(RPG) root@C.13666495:/home/kubig/RPG-DiffusionMaster$ python RPG.py
Loading pipeline components...: 100%|██████████████████| 7/7 [00:02<00:00,  2.71it/s]
Using hardcoded values instead of GPT API.
Final split ratio: 1,2,1;1,1,1
Regional Prompt: Captures the dog with an engaging head tilt, eyes bright with curiosity, and a playful grin, with the letter Thumbnail in the
left top part. BREAK Shows the dog lying down, paws folded, with a calm expression, its fur slightly ruffled. BREAK Presents the dog mid-play,
leaping with ears perked up, full of energetic joy. BREAK Features the dog sitting serenely, a gentle breeze rustling its fur, with a calm and
serene gaze.
split 1,2,1;1,1,1
split_ratio2r [1.0, 1.0]
split_ratio2 [[2.0, 1.0], [1.0, 1.0]]
[[0.5, 0.5], [0.5, 0.5]]
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
5
prompt_embeds shape torch.Size([1, 385, 2048])
Traceback (most recent call last):
  File "/home/kubig/RPG-DiffusionMaster/RPG.py", line 29, in <module>
    images = pipe(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/kubig/RPG-DiffusionMaster/RegionalDiffusion_xl.py", line 1295, in __call__
    print("frames :", frames.shape)
AttributeError: 'list' object has no attribute 'shape'
(RPG) root@C.13666495:/home/kubig/RPG-DiffusionMaster$ python RPG.py
Loading pipeline components...: 100%|██████████████████| 7/7 [00:02<00:00,  2.71it/s]
Using hardcoded values instead of GPT API.
Final split ratio: 1,2,1;1,1,1
Regional Prompt: Captures the dog with an engaging head tilt, eyes bright with curiosity, and a playful grin, with the letter Thumbnail in the
left top part. BREAK Shows the dog lying down, paws folded, with a calm expression, its fur slightly ruffled. BREAK Presents the dog mid-play,
leaping with ears perked up, full of energetic joy. BREAK Features the dog sitting serenely, a gentle breeze rustling its fur, with a calm and
serene gaze.
split 1,2,1;1,1,1
split_ratio2r [1.0, 1.0]
split_ratio2 [[2.0, 1.0], [1.0, 1.0]]
[[0.5, 0.5], [0.5, 0.5]]
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
5
prompt_embeds shape torch.Size([1, 385, 2048])
Traceback (most recent call last):
  File "/home/kubig/RPG-DiffusionMaster/RPG.py", line 29, in <module>
    images = pipe(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/kubig/RPG-DiffusionMaster/RegionalDiffusion_xl.py", line 1295, in __call__
    print("frames :", frames[0].shape)
AttributeError: 'Image' object has no attribute 'shape'
(RPG) root@C.13666495:/home/kubig/RPG-DiffusionMaster$ python RPG.py
Loading pipeline components...: 100%|██████████████████| 7/7 [00:02<00:00,  2.68it/s]
Using hardcoded values instead of GPT API.
Final split ratio: 1,2,1;1,1,1
Regional Prompt: Captures the dog with an engaging head tilt, eyes bright with curiosity, and a playful grin, with the letter Thumbnail in the
left top part. BREAK Shows the dog lying down, paws folded, with a calm expression, its fur slightly ruffled. BREAK Presents the dog mid-play,
leaping with ears perked up, full of energetic joy. BREAK Features the dog sitting serenely, a gentle breeze rustling its fur, with a calm and
serene gaze.
split 1,2,1;1,1,1
split_ratio2r [1.0, 1.0]
split_ratio2 [[2.0, 1.0], [1.0, 1.0]]
[[0.5, 0.5], [0.5, 0.5]]
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
5
prompt_embeds shape torch.Size([1, 385, 2048])
split: [['1', '2', '1'], ['1', '1', '1']]
Combined image saved at: visualize/combined_image.png
Images inside regional_encode_image: [<PIL.Image.Image image mode=RGB size=560x373 at 0x7A47645AEBB0>, <PIL.Image.Image image mode=RGB size=578
x385 at 0x7A47645AEA90>, <PIL.Image.Image image mode=RGB size=540x315 at 0x7A47645AEC40>, <PIL.Image.Image image mode=RGB size=600x400 at 0x7A4
7645AEB80>]
Processing image: <PIL.Image.Image image mode=RGB size=560x373 at 0x7A47645AEBB0>
Output hidden states is True.
Image hidden states shape: torch.Size([1, 257, 1024])
Appended image_enc_hidden_states to image_embeds_list.
Appended uncond_image_enc_hidden_states to uncond_image_embeds_list.
Processing image: <PIL.Image.Image image mode=RGB size=578x385 at 0x7A47645AEA90>
Output hidden states is True.
Image hidden states shape: torch.Size([1, 257, 1024])
Appended image_enc_hidden_states to image_embeds_list.
Appended uncond_image_enc_hidden_states to uncond_image_embeds_list.
Processing image: <PIL.Image.Image image mode=RGB size=540x315 at 0x7A47645AEC40>
Output hidden states is True.
Image hidden states shape: torch.Size([1, 257, 1024])
Appended image_enc_hidden_states to image_embeds_list.
Appended uncond_image_enc_hidden_states to uncond_image_embeds_list.
Processing image: <PIL.Image.Image image mode=RGB size=600x400 at 0x7A47645AEB80>
Output hidden states is True.
Image hidden states shape: torch.Size([1, 257, 1024])
Appended image_enc_hidden_states to image_embeds_list.
Appended uncond_image_enc_hidden_states to uncond_image_embeds_list.
Length of image_embeds_list: 4
Length of uncond_image_embeds_list: 4
Decoded image from latent saved at: visualize/decoded_image_from_latent.png
Standard latents shape: torch.Size([1, 4, 128, 128])
prompt_embeds shape torch.Size([2, 385, 2048])
add text_embeds shape torch.Size([2, 1280])
  0%|                                                         | 0/80 [00:00<?, ?it/s][Step 0] Latent model input mean: 0.0745849609375, std: 0.
884765625
added torch.Size([1, 1028, 1024])
[Step 0] Noise pred mean: 0.006378173828125, std: 0.9287109375
[Step 0] Noise pred (uncond) mean: 0.0046234130859375, std: 0.9287109375
[Step 0] Noise pred (text) mean: 0.0081329345703125, std: 0.92919921875
[Step 0] Noise pred (final) mean: 0.0291748046875, std: 1.005859375
[Step 0] Updated latents mean: 0.07965087890625, std: 0.88525390625
Intermediate decoded image saved at step 0.
  1%|▌                                                | 1/80 [00:01<01:39,  1.25s/it][Step 1] Latent model input mean: 0.07965087890625, std: 0
.88525390625
added torch.Size([1, 1028, 1024])
[Step 1] Noise pred mean: 0.0014562606811523438, std: 0.92236328125
[Step 1] Noise pred (uncond) mean: -0.00023221969604492188, std: 0.9208984375
[Step 1] Noise pred (text) mean: 0.0031452178955078125, std: 0.923828125
[Step 1] Noise pred (final) mean: 0.023406982421875, std: 0.982421875
[Step 1] Updated latents mean: 0.08636474609375, std: 0.88671875
  2%|█▏                                               | 2/80 [00:01<01:05,  1.19it/s][Step 2] Latent model input mean: 0.08636474609375, std: 0
.88671875
added torch.Size([1, 1028, 1024])
[Step 2] Noise pred mean: -0.004150390625, std: 0.91748046875
[Step 2] Noise pred (uncond) mean: -0.006481170654296875, std: 0.916015625
[Step 2] Noise pred (text) mean: -0.00182342529296875, std: 0.91845703125
[Step 2] Noise pred (final) mean: 0.0261077880859375, std: 0.982421875
[Step 2] Updated latents mean: 0.0931396484375, std: 0.89306640625
  4%|█▊                                               | 3/80 [00:02<00:54,  1.40it/s][Step 3] Latent model input mean: 0.0931396484375, std: 0.
89306640625
added torch.Size([1, 1028, 1024])
[Step 3] Noise pred mean: -0.01276397705078125, std: 0.90869140625
[Step 3] Noise pred (uncond) mean: -0.01500701904296875, std: 0.90771484375
[Step 3] Noise pred (text) mean: -0.0105133056640625, std: 0.91015625
[Step 3] Noise pred (final) mean: 0.0164337158203125, std: 0.986328125
[Step 3] Updated latents mean: 0.1026611328125, std: 0.904296875
  5%|██▍                                              | 4/80 [00:02<00:49,  1.54it/s][Step 4] Latent model input mean: 0.1026611328125, std: 0.
904296875
added torch.Size([1, 1028, 1024])
[Step 4] Noise pred mean: -0.0216064453125, std: 0.900390625
[Step 4] Noise pred (uncond) mean: -0.0230712890625, std: 0.8994140625
[Step 4] Noise pred (text) mean: -0.0201416015625, std: 0.9013671875
[Step 4] Noise pred (final) mean: -0.002574920654296875, std: 0.9580078125
[Step 4] Updated latents mean: 0.1163330078125, std: 0.92041015625
  6%|███                                              | 5/80 [00:03<00:46,  1.62it/s][Step 5] Latent model input mean: 0.1163330078125, std: 0.
92041015625
added torch.Size([1, 1028, 1024])
[Step 5] Noise pred mean: -0.027496337890625, std: 0.89208984375
[Step 5] Noise pred (uncond) mean: -0.0281829833984375, std: 0.89111328125
[Step 5] Noise pred (text) mean: -0.02679443359375, std: 0.89306640625
[Step 5] Noise pred (final) mean: -0.01849365234375, std: 0.935546875
[Step 5] Updated latents mean: 0.133544921875, std: 0.943359375
Intermediate decoded image saved at step 5.
  8%|███▋                                             | 6/80 [00:04<01:01,  1.20it/s][Step 6] Latent model input mean: 0.133544921875, std: 0.9
43359375
added torch.Size([1, 1028, 1024])
[Step 6] Noise pred mean: -0.031646728515625, std: 0.8828125
[Step 6] Noise pred (uncond) mean: -0.031982421875, std: 0.88232421875
[Step 6] Noise pred (text) mean: -0.03131103515625, std: 0.8837890625
[Step 6] Noise pred (final) mean: -0.02734375, std: 0.9267578125
[Step 6] Updated latents mean: 0.15380859375, std: 0.974609375
  9%|████▎                                            | 7/80 [00:05<00:54,  1.35it/s][Step 7] Latent model input mean: 0.15380859375, std: 0.97
4609375
added torch.Size([1, 1028, 1024])
[Step 7] Noise pred mean: -0.03369140625, std: 0.873046875
[Step 7] Noise pred (uncond) mean: -0.033966064453125, std: 0.8720703125
[Step 7] Noise pred (text) mean: -0.033447265625, std: 0.8740234375
[Step 7] Noise pred (final) mean: -0.030242919921875, std: 0.90625
[Step 7] Updated latents mean: 0.1767578125, std: 1.013671875
 10%|████▉                                            | 8/80 [00:05<00:49,  1.46it/s][Step 8] Latent model input mean: 0.1767578125, std: 1.013
671875
added torch.Size([1, 1028, 1024])
[Step 8] Noise pred mean: -0.034393310546875, std: 0.8623046875
[Step 8] Noise pred (uncond) mean: -0.03436279296875, std: 0.861328125
[Step 8] Noise pred (text) mean: -0.034393310546875, std: 0.86328125
[Step 8] Noise pred (final) mean: -0.03448486328125, std: 0.890625
[Step 8] Updated latents mean: 0.203369140625, std: 1.060546875
 11%|█████▌                                           | 9/80 [00:06<00:45,  1.55it/s][Step 9] Latent model input mean: 0.203369140625, std: 1.0
60546875
added torch.Size([1, 1028, 1024])
[Step 9] Noise pred mean: -0.034149169921875, std: 0.8515625
[Step 9] Noise pred (uncond) mean: -0.0340576171875, std: 0.85009765625
[Step 9] Noise pred (text) mean: -0.03424072265625, std: 0.8525390625
[Step 9] Noise pred (final) mean: -0.035369873046875, std: 0.87744140625
[Step 9] Updated latents mean: 0.233642578125, std: 1.1171875
 12%|██████                                          | 10/80 [00:06<00:43,  1.62it/s][Step 10] Latent model input mean: 0.233642578125, std: 1.
1171875
added torch.Size([1, 1028, 1024])
 12%|██████                                          | 10/80 [00:07<00:52,  1.33it/s]
Traceback (most recent call last):
  File "/home/kubig/RPG-DiffusionMaster/RPG.py", line 29, in <module>
    images = pipe(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/kubig/RPG-DiffusionMaster/RegionalDiffusion_xl.py", line 1500, in __call__
    print(f"[Step {i}] Noise pred mean: {noise_pred.mean().item()}, std: {noise_pred.std().item()}")
KeyboardInterrupt

(RPG) root@C.13666495:/home/kubig/RPG-DiffusionMaster$ tmux capture-pane -pS -500 > /home/kubig/RPG-DiffusionMaster/recent_history
(RPG) root@C.13666495:/home/kubig/RPG-DiffusionMaster$ python RPG.py
Loading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████| 7/7 [00:02<00:00,  2.70it/s]
Using hardcoded values instead of GPT API.
Final split ratio: 1,2,1;1,1,1
Regional Prompt: Captures the dog with an engaging head tilt, eyes bright with curiosity, and a playful grin, with the letter Thumbnail in the
left top part. BREAK Shows the dog lying down, paws folded, with a calm expression, its fur slightly ruffled. BREAK Presents the dog mid-play,
leaping with ears perked up, full of energetic joy. BREAK Features the dog sitting serenely, a gentle breeze rustling its fur, with a calm and
serene gaze.
split 1,2,1;1,1,1
split_ratio2r [1.0, 1.0]
split_ratio2 [[2.0, 1.0], [1.0, 1.0]]
[[0.5, 0.5], [0.5, 0.5]]
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
5
prompt_embeds shape torch.Size([1, 385, 2048])
split: [['1', '2', '1'], ['1', '1', '1']]
Combined image saved at: visualize/combined_image.png
Images inside regional_encode_image: [<PIL.Image.Image image mode=RGB size=560x373 at 0x77448BDA2C10>, <PIL.Image.Image image mode=RGB size=578
x385 at 0x77448BDA2AF0>, <PIL.Image.Image image mode=RGB size=540x315 at 0x77448BDA2CA0>, <PIL.Image.Image image mode=RGB size=600x400 at 0x774
48BDA2BE0>]
Processing image: <PIL.Image.Image image mode=RGB size=560x373 at 0x77448BDA2C10>
Output hidden states is True.
Image hidden states shape: torch.Size([1, 257, 1024])
Appended image_enc_hidden_states to image_embeds_list.
Appended uncond_image_enc_hidden_states to uncond_image_embeds_list.
Processing image: <PIL.Image.Image image mode=RGB size=578x385 at 0x77448BDA2AF0>
Output hidden states is True.
Image hidden states shape: torch.Size([1, 257, 1024])
Appended image_enc_hidden_states to image_embeds_list.
Appended uncond_image_enc_hidden_states to uncond_image_embeds_list.
Processing image: <PIL.Image.Image image mode=RGB size=540x315 at 0x77448BDA2CA0>
Output hidden states is True.
Image hidden states shape: torch.Size([1, 257, 1024])
Appended image_enc_hidden_states to image_embeds_list.
Appended uncond_image_enc_hidden_states to uncond_image_embeds_list.
Processing image: <PIL.Image.Image image mode=RGB size=600x400 at 0x77448BDA2BE0>
Output hidden states is True.
Image hidden states shape: torch.Size([1, 257, 1024])
Appended image_enc_hidden_states to image_embeds_list.
Appended uncond_image_enc_hidden_states to uncond_image_embeds_list.
Length of image_embeds_list: 4
Length of uncond_image_embeds_list: 4
Decoded image from latent saved at: visualize/decoded_image_from_latent.png
Standard latents shape: torch.Size([1, 4, 128, 128])
real prompt_embeds shape torch.Size([2, 385, 2048])
image_prompt_embeds shape torch.Size([1, 1028, 1024])
add text_embeds shape torch.Size([2, 1280])
  0%|                                                                                                                   | 0/80 [00:00<?, ?it/s]
[Step 0] Latent model input mean: 0.0745849609375, std: 0.884765625
added torch.Size([1, 1028, 1024])
  0%|                                                                                                                   | 0/80 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/kubig/RPG-DiffusionMaster/RPG.py", line 29, in <module>
    images = pipe(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/kubig/RPG-DiffusionMaster/RegionalDiffusion_xl.py", line 1491, in __call__
    noise_pred = self.unet(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/diffusers/models/unets/unet_2d_condition.py", line 1216, in forward
    sample, res_samples = downsample_block(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/diffusers/models/unets/unet_2d_blocks.py", line 1279, in forward
    hidden_states = attn(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/diffusers/models/transformers/transformer_2d.py", line 397, in forward
    hidden_states = block(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/diffusers/models/attention.py", line 366, in forward
    attn_output = self.attn2(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/kubig/RPG-DiffusionMaster/cross_attention.py", line 149, in forward
    conn,conp = contexts.chunk(2)
ValueError: not enough values to unpack (expected 2, got 1)
(RPG) root@C.13666495:/home/kubig/RPG-DiffusionMaster$ tmux capture-pane -pS -500 > /home/kubig/RPG-DiffusionMaster/recent_history
(RPG) root@C.13666495:/home/kubig/RPG-DiffusionMaster$ tmux capture-pane -pS -500 > /home/kubig/RPG-DiffusionMaster/recent_history
(RPG) root@C.13666495:/home/kubig/RPG-DiffusionMaster$ python RPG.py
Loading pipeline components...: 100%|████████████████████████████████████████████████████████████████████████████| 7/7 [00:02<00:00,  2.68it/s]
Using hardcoded values instead of GPT API.
Final split ratio: 1,2,1;1,1,1
Regional Prompt: Captures the dog with an engaging head tilt, eyes bright with curiosity, and a playful grin, with the letter Thumbnail in the
left top part. BREAK Shows the dog lying down, paws folded, with a calm expression, its fur slightly ruffled. BREAK Presents the dog mid-play,
leaping with ears perked up, full of energetic joy. BREAK Features the dog sitting serenely, a gentle breeze rustling its fur, with a calm and
serene gaze.
split 1,2,1;1,1,1
split_ratio2r [1.0, 1.0]
split_ratio2 [[2.0, 1.0], [1.0, 1.0]]
[[0.5, 0.5], [0.5, 0.5]]
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 77, 768])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
 defined prompt embeds shape torch.Size([1, 1280])
5
prompt_embeds shape torch.Size([1, 385, 2048])
split: [['1', '2', '1'], ['1', '1', '1']]
Combined image saved at: visualize/combined_image.png
Images inside regional_encode_image: [<PIL.Image.Image image mode=RGB size=560x373 at 0x72E1958B9C40>, <PIL.Image.Image image mode=RGB size=578
x385 at 0x72E1958B9B20>, <PIL.Image.Image image mode=RGB size=540x315 at 0x72E1958B9CD0>, <PIL.Image.Image image mode=RGB size=600x400 at 0x72E
1958B9C10>]
Processing image: <PIL.Image.Image image mode=RGB size=560x373 at 0x72E1958B9C40>
Output hidden states is True.
Image hidden states shape: torch.Size([1, 257, 1024])
Appended image_enc_hidden_states to image_embeds_list.
Appended uncond_image_enc_hidden_states to uncond_image_embeds_list.
Processing image: <PIL.Image.Image image mode=RGB size=578x385 at 0x72E1958B9B20>
Output hidden states is True.
Image hidden states shape: torch.Size([1, 257, 1024])
Appended image_enc_hidden_states to image_embeds_list.
Appended uncond_image_enc_hidden_states to uncond_image_embeds_list.
Processing image: <PIL.Image.Image image mode=RGB size=540x315 at 0x72E1958B9CD0>
Output hidden states is True.
Image hidden states shape: torch.Size([1, 257, 1024])
Appended image_enc_hidden_states to image_embeds_list.
Appended uncond_image_enc_hidden_states to uncond_image_embeds_list.
Processing image: <PIL.Image.Image image mode=RGB size=600x400 at 0x72E1958B9C10>
Output hidden states is True.
Image hidden states shape: torch.Size([1, 257, 1024])
Appended image_enc_hidden_states to image_embeds_list.
Appended uncond_image_enc_hidden_states to uncond_image_embeds_list.
Length of image_embeds_list: 4
Length of uncond_image_embeds_list: 4
Decoded image from latent saved at: visualize/decoded_image_from_latent.png
Standard latents shape: torch.Size([1, 4, 128, 128])
real prompt_embeds shape torch.Size([2, 385, 2048])
image_prompt_embeds shape torch.Size([2, 1028, 1024])
add text_embeds shape torch.Size([2, 1280])
  0%|                                                                                                                   | 0/80 [00:00<?, ?it/s]
[Step 0] Latent model input mean: 0.0745849609375, std: 0.884765625
added torch.Size([1, 1028, 1024])
  0%|                                                                                                                   | 0/80 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/kubig/RPG-DiffusionMaster/RPG.py", line 29, in <module>
    images = pipe(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/kubig/RPG-DiffusionMaster/RegionalDiffusion_xl.py", line 1491, in __call__
    noise_pred = self.unet(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/diffusers/models/unets/unet_2d_condition.py", line 1216, in forward
    sample, res_samples = downsample_block(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/diffusers/models/unets/unet_2d_blocks.py", line 1279, in forward
    hidden_states = attn(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/diffusers/models/transformers/transformer_2d.py", line 397, in forward
    hidden_states = block(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/diffusers/models/attention.py", line 366, in forward
    attn_output = self.attn2(
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/kubig/RPG-DiffusionMaster/cross_attention.py", line 153, in forward
    opx = matsepcalc(px, conp, True, 2)
  File "/home/kubig/RPG-DiffusionMaster/cross_attention.py", line 92, in matsepcalc
    out = main_forward_diffusers(module, x, context, divide,userpp =True,isxl = self.isxl)
  File "/home/kubig/RPG-DiffusionMaster/cross_attention.py", line 26, in main_forward_diffusers
    key = module.to_k(context)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/opt/conda/envs/RPG/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (104x1024 and 2048x640)
(RPG) root@C.13666495:/home/kubig/RPG-DiffusionMaster$ tmux capture-pane -pS -500 > /home/kubig/RPG-DiffusionMaster/recent_history

